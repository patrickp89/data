---
title: "Data analysis: ebuy's sales data"
bibliography: "bibliographies/literature.bib"
output:
  bookdown::html_document2:
    toc: yes
    theme: spacelab
---
```{r setRngSeed, echo=FALSE}
# To always get (and report) the same cross-validation results (and any
# other random-based things) we initialize the RNG with a fixed seed:
set.seed(26769)
```

# Data Manipulation
```{r includes01, echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(knitr)
library(kableExtra)
library(usmap)
```

## Loading the Data and Recoding Variables
We first loaded the data, picked the columns that we were interested in, and forced some of the variables to be categorical.
```{r pickColumnsForceCat01, echo=FALSE}
orderDf <- read.csv(file="../data/interim/orders/orders_cleaned.csv", sep=",", na.strings=c("?","NA", "NULL"))
partialOrdersDf <- orderDf[c("Product_Family_ID",
                             "Order_Line_Day_of_Week",
                             "Order_Line_Hour_of_Day",
                             "Order_Line_Amount",
                             "Product_ID",
                             "Gender",
                             "US_State",
                             "Age",
                             "Order_Day_of_Week")]

partialOrdersDf$Product_Family_ID <- as.factor(partialOrdersDf$Product_Family_ID)
partialOrdersDf$Order_Line_Day_of_Week <- as.factor(partialOrdersDf$Order_Line_Day_of_Week)
partialOrdersDf$Order_Line_Hour_of_Day <- as.factor(partialOrdersDf$Order_Line_Hour_of_Day)
partialOrdersDf$Gender <- as.factor(partialOrdersDf$Gender)
partialOrdersDf$US_State <- as.factor(partialOrdersDf$US_State)
partialOrdersDf$Age <- as.numeric(partialOrdersDf$Age)

#partialOrdersDf$Order_Session_ID <- factor(partialOrdersDf$Order_Session_ID)
#partialOrdersDf$Product_Object_ID <- factor(partialOrdersDf$Product_Object_ID)
#partialOrdersDf$Product_ID <- factor(partialOrdersDf$Product_ID)
#partialClickstreamDf$Session_ID <- factor(partialClickstreamDf$Session_ID)

summary(partialOrdersDf)
```

## Cleaning the Data


# Summary Statistics
For ebuy it should be highly interesting to find out, which of their products and product families are the ones that sell best.
```{r mostOrderedProductsTable, echo=FALSE}
# TODO: these 2 tables do float, but are awkwardly close to the page borders --> center them!
partialOrdersDf %>%
  drop_na(Product_ID) %>%
  count(Product_ID) %>%
  arrange(desc(n)) %>%
  slice(1:6) %>%
  kable(.) %>%
  kable_styling(full_width=FALSE, position="float_left")
```
```{r mostOrderedProductFamiliesTable, echo=FALSE}
partialOrdersDf %>%
  drop_na(Product_Family_ID) %>%
  count(Product_Family_ID) %>%
  arrange(desc(n)) %>%
  slice(1:6) %>%
  kable(.) %>%
  kable_styling(full_width=FALSE, position="right")
```

Orders at ebuy's online shop might be distributed differently among the different days of the week. To examine, whether this is the case, we plotted a frequency distribution of all orders over the days of week. We first reordered the days in the canonical way (i.e. Monday, Tuesday, Wednesday, ...) and then counted and plotted the order frequencies against them (figure \@ref(fig:orderDayBarPlot1)). Clearly, the most orders are placed during the weekdays (peaking on Wednesdays), the least orders on sunday.
```{r orderDayBarPlot1, echo=FALSE, fig.cap="Distribution of orders among different days of the week"}
partialOrdersDf %>%
  mutate(Order_Day_of_Week = factor(Order_Day_of_Week, levels=c("Monday", "Tuesday", "Wednesday",
                                                                "Thursday", "Friday", "Saturday",
                                                                "Sunday"))) %>%
  count(Order_Day_of_Week) %>%
  slice(1:7) %>%
ggplot(data = .) +
  geom_bar(mapping = aes(x = Order_Day_of_Week, y = n), stat = "identity")
```

```{r}
clickstream_data <- read.csv(file="../data/interim/clickstream/clickstream_cleaned.csv", sep=",", na.strings=c("NA"))
```

## Clickstream

```{r summarytable1, echo=FALSE}

table_data <- clickstream_data %>%
  select(Request_Processing_Time, Request_Sequence, REQUEST_HOUR_OF_DAY) %>%
  transmute(mean_RPT = mean(Request_Processing_Time), mean_RS = mean(Request_Sequence), mean_HOD = mean(REQUEST_HOUR_OF_DAY),
            sd_RPT = sd(Request_Processing_Time), sd_RS = sd(Request_Sequence), sd_HOD = sd(REQUEST_HOUR_OF_DAY),
            min_RPT = min(Request_Processing_Time), min_RS = min(Request_Sequence), min_HOD = min(REQUEST_HOUR_OF_DAY),
            max_RPT = max(Request_Processing_Time), max_RS = max(Request_Sequence), max_HOD = max(REQUEST_HOUR_OF_DAY))

table_data <- table_data[1,] %>%
  gather("column", "mean", 1:3) %>% 
  gather("sd_data", "sd", 1:3) %>% 
  gather("min_data", "min", 1:3) %>% 
  gather("max_data", "max", 1:3) %>%
  mutate(column = sapply(strsplit(column, "_"), "[", 2),
         sd_data = sapply(strsplit(sd_data, "_"), "[", 2),
         min_data = sapply(strsplit(min_data, "_"), "[", 2),
         max_data = sapply(strsplit(max_data, "_"), "[", 2)) %>%
  filter(column == sd_data & column == min_data & column == max_data) %>%
  select(column, mean, sd, min, max)

kable(table_data)
  
```


# Plots

## Clickstream

```{r heatmap, echo=FALSE}


map_data <- clickstream_data %>%
  select(US_State) %>%
  group_by(US_State) %>%
  summarise(Request_Count = n(), state = first(US_State))

plot_usmap(data = map_data, values = "Request_Count", labels = TRUE) +
  scale_fill_continuous(low = "white", high = "red", name = "average Order Amount") +
  labs(title = "How much money do customers from different states spend?",
       subtitle = "Average order amount split by US State") +
  theme(plot.title = element_text(face = "bold", size = 15, hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5, size = 12, face = "italic"),
        legend.position = "bottom", legend.justification = "center",
        legend.background = element_blank())

```

```{r requests, echo = FALSE}

data_requests_products <- clickstream_data %>%
  select(Product_ID) %>%
  filter(Product_ID != "?") %>%
  group_by(Product_ID) %>%
  summarise(Request_Count = n())

ggplot(data_requests_products) +
  geom_line(aes(x = reorder(Product_ID, desc(Request_Count)), y = Request_Count, group = 1)) +
  labs(x = "Products", y = "Requests",
       title = "How many times was each product accessed?",
       subtitle = "Requests per product")  +
  theme_classic() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) +
  scale_y_continuous(expand = c(0,0))

```

```{r sessionduration, echo=FALSE}
  duration_data <- clickstream_data %>%
  select(Session_ID, Request_Sequence, Request_Date, Request_Date_Time, Session_First_Request_Date, Session_First_Request_Date_Time) %>%
  unite(col = "Request_Timestamp",c("Request_Date", "Request_Date_Time"), sep = " ", remove = TRUE) %>%
  unite(col="Session_First_Request_Timestamp", c("Session_First_Request_Date","Session_First_Request_Date_Time"), sep=" ", remove=TRUE) %>%
  mutate(Request_Timestamp = as.POSIXct(Request_Timestamp, format="%Y-%m-%d %H\\:%M\\:%S")) %>%
  mutate(Session_First_Request_Timestamp = as.POSIXct(Session_First_Request_Timestamp, format="%Y-%m-%d %H\\:%M\\:%S")) %>%
  group_by(Session_ID) %>%
  summarise(Pages_Visited=n(),Session_First_Request=min(Session_First_Request_Timestamp),Session_Last_Request=max(Request_Timestamp)) %>%
  mutate(Session_Duration = as.numeric(difftime(Session_Last_Request, Session_First_Request,units = "secs",))) %>%
  group_by(Pages_Visited) %>%
  summarise(avg_Session_Duration = mean(Session_Duration), Session_Count = n_distinct(Session_ID)) %>%
  filter(Session_Count > 2 & Pages_Visited > 1)
  
  ggplot(data = duration_data) +
  geom_point(aes(x = Pages_Visited, y = avg_Session_Duration, size = Session_Count)) +
  labs(x = "Pages Visited", y = "average Session Duration (seconds)", size = "Sessions",
       title = "How long do customers stay on the site?",
       subtitle = "Average session duration per session page visits") +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

## What are the most popular brands for men and women?
### Women

```{r include=FALSE}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(stringr)
orders = read.csv("../data/interim/orders/orders_cleaned.csv")
orders = orders %>% select(Customer_ID, Order_Line_Session_ID, Order_ID, Customer_ID, Age, Gender, Product_Level_2_Path, Order_Status, Order_Line_Quantity)
orders$Brand = unlist(lapply(orders$Product_Level_2_Path, FUN = function(path){return(str_match(path, "\\/.*\\/.*\\/(.*)")[,2])}))
```

To get an overview over what brands your customers like the most we vizualized the popularity of the brands based on how often they get purchased.
```{r include= FALSE}
mostPopularFemaleBrands = orders %>% 
  filter(Gender == "Female") %>%
  filter(!is.na(Brand)) %>%
  group_by(Brand) %>%
  summarise(Order_Amount = sum(Order_Line_Quantity)) %>%
  arrange(desc(Order_Amount)) %>%
  head(10)
```

```{r echo=FALSE}
mostPopularFemaleBrands
```

```{r echo=FALSE}
ggplot(mostPopularFemaleBrands, mapping = aes(x = reorder(Brand, Order_Amount), y = Order_Amount, fill = Brand)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Most Popular Female Brands", x = "Brand", y = "Number of Purchases")
```

### Men
```{r include=FALSE}
mostPopularMaleBrands = orders %>%
  filter(Gender == "Male") %>%
  filter(!is.na(Brand)) %>%
  group_by(Brand) %>%
  summarise(Order_Amount = sum(Order_Line_Quantity)) %>%
  arrange(desc(Order_Amount)) %>%
  head(10)
```


```{r echo=FALSE}
ggplot(mostPopularMaleBrands, mapping = aes(x = reorder(Brand, Order_Amount), y = Order_Amount, fill = Brand)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Most Popular Male Brands", x = "Brand", y = "Number of Purchases")
```


# Comparison of Groups
After loading and examining the dataset containing ebuy's experiment results we picked all columns of interest (e.g. deciding to keep the first ID column only).
```{r loadexpdata, echo=FALSE}
experimentDf <- read.csv(file="../experiment/experimental_results.csv", sep=",", na.strings=c("NA"))
partialExperimentDf <- experimentDf[c("id_",
                                      "ranking_based",
                                      "random_recommendations",
                                      "profit_oriented")]

names(partialExperimentDf)[1] <- "id"
partialExperimentDf$id <- as.factor(partialExperimentDf$id)
summary(partialExperimentDf)
```
## Broad overview of distributions
```{r include=FALSE}
data <- read.csv("../experiment/experimental_results.csv")
recommender_to_sales <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("Id", "Recommender", "Sales"))
index <- 1
for(row in 1:nrow(data)){
  if(!is.na(data[row, "ranking_based"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "ranking_based", as.numeric(data[row, "ranking_based"]))
    index <- index+1
  }
  if(!is.na(data[row, "random_recommendations"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "random_recommendations", as.numeric(data[row, "random_recommendations"]))
    index <- index+1
  }
  if(!is.na(data[row, "profit_oriented"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "profit_oriented", as.numeric(data[row, "profit_oriented"]))
    index <- index+1
  }
}
recommender_to_sales$Id <- NULL

```

```{r echo=FALSE}
ggplot(recommender_to_sales %>% filter(Recommender=="ranking_based"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density", title="Ranking-Based Recommender: Sales Density and Mean")
```

```{r echo=FALSE}
ggplot(recommender_to_sales %>% filter(Recommender=="profit_oriented"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density", title="Profit-Oriented Recommender: Sales Density and Mean")
```

```{r echo=FALSE}
ggplot(recommender_to_sales %>% filter(Recommender=="random_recommendations"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density", title="Random Recommender: Sales Density and Mean")
```

```{r echo=FALSE}
result <- group_by(recommender_to_sales, Recommender) %>%
  summarise(
    Count = n(),
    Mean = mean(Sales, na.rm = TRUE),
    Sd = sd(Sales, na.rm = TRUE),
    Se = Sd/sqrt(Count),
    CiMult = qt(0.975, Count-1),
    Ci = Se * CiMult
  )

# visualize CI
ggplot(result, aes(x=Recommender, y=Mean, group=1)) +
  geom_point(alpha=0.52) +
  geom_errorbar(width=.1, aes(ymin=Mean-Ci, ymax=Mean+Ci), colour="darkred") + labs(x="Recommender System",y= "Sales", title="Recommender Confidence Intervals")
```

It seems like the profit-oriented recommender performs the best (mu=21.75) while the random recommender (mu=18.5) and the ranking based (mu=19.0) recommender seem to perform quite similar. Furthermore the confidence interval of the profit-oriented recommender is very narrow while the confidence interval of the ranking-based and random recommender system are wider and also overlap. Therefore we should definitely do a statisitcal test to prove if the means are significantly different or not. 
We will first compare the profit-oriented and ranking-based recommender each with the random recommender to see if the recommenders perform significantly better than a random recommender. Before doing so we first check if the distributions of the recommenders are normally distributed so we can perform a t-test. In order to do so we used the Shapioro-Wilk normality test.

## Shapiro-Wilk Normality Test

```{r echo = FALSE}
with(recommender_to_sales, shapiro.test(Sales[Recommender == "profit_oriented"]))
with(recommender_to_sales, shapiro.test(Sales[Recommender == "ranking_based"]))
with(recommender_to_sales, shapiro.test(Sales[Recommender == "random_recommendations"]))
```
All the p-values are smaller than the significance level of 0.05 which means that the distributions are not normally distributed. Therefore we used the Unpaired Two-Samples Wilcoxon test to compare the means of the different recommenders.

## Wilcoxon Test

```{r echo = FALSE}
# perform wulxoc test
ranking_based_sales <- recommender_to_sales %>% filter(Recommender == "ranking_based")
profit_oriented_sales <- recommender_to_sales %>% filter(Recommender == "profit_oriented")
random_recommendations_sales <- recommender_to_sales %>% filter(Recommender == "random_recommendations")

wilcox.test(profit_oriented_sales$Sales, random_recommendations_sales$Sales, alternative = "two.sided")
wilcox.test(ranking_based_sales$Sales, random_recommendations_sales$Sales, alternative = "two.sided")
wilcox.test(profit_oriented_sales$Sales, ranking_based_sales$Sales, alternative = "two.sided")
```

We then compared the results of both the ranking-based as well as the profit-oriented recommender systems against the results of the baseline recommender system (that selects products randomly). We first compared the profit-oriented recommender to the random recommender (baseline recommender). The results of the tests show that the means of the both recommenders are not the same (p<0.00001). Surprisingly, the same hypothesis could not be rejected for the comparison of the ranking-based and the baseline recommender system (p=0.2008). As the profit-oriented recommender systems mean value (21.75) is greater than the baseline mean (17.91), ebuy should preferably use the profit-oriented one.

# Models and Prediction
```{r includes02, echo=FALSE, message=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
```

## Training the Model{#subsection-ttm}
As ebuy's products fall into certain product families, we trained a model that allows to predict for a new customer from which product family he will purchase a product. We first grew a large tree with a complexity parameter of cp=0.00001 (i.e. splits had to decrease the lack of fit by a factor of 0.00001).
```{r modelTraining1, echo=FALSE}
tree <- rpart(Product_Family_ID ~ Order_Line_Day_of_Week + Gender + Age,
              method="class",
              data=partialOrdersDf,
              cp=0.00001)
```
We then examined the results of a 10-fold cross-validation by plotting the complexity parameter vs. its cross-validated error (as shown in figure \@ref(fig:cpplottree1)).
```{r cpplottree1, echo=FALSE, fig.cap="Product Family ID: Complexity parameter vs. cross-validated error"}
plotcp(tree)
```

To prevent over-fitting, @kabacoff2015 suggests to choose the leftmost cp value below the dotted line (here: cp=0.0007): a tree of that size (here: 45 splits) is the smallest tree whose cross-validated error is whithin one standard error of the minimum cross-validated error value. We therfore pruned the tree to its new size and pretty-printed the resulting tree (figure \@ref(fig:prunedtree1)).
```{r prunedtree1, echo=FALSE, fig.cap="Product Family ID: Pruned Classification Tree"}
prune(tree, cp=0.0007) %>%
  prp(.)
```

Next we computed a 10-fold cross-validated linear regression model that helped us understand, how the different properties of customers affected the amount of money they would spend in total.
```{r linReg1, echo=FALSE}
# enable cross validation and set the number of folds:
ctrl <- trainControl(method = "cv",
                     number = 10)

# preform a linear regression:
linreg1 <- train(Order_Line_Amount ~ Age + Gender,
                 data = partialOrdersDf,
                 trControl = ctrl,
                 na.action  = na.pass,
                 method = "lm")
linreg1
linreg1$finalModel
```

## Predictions
Having trained the models in \@ref(subsection-ttm) ebuy could predict the purchasing behaviour of future customers. Given a previously unseen customer, ebuy could for instance now predict the product family which she is going to purchase from.
```{r prediction1, eval=FALSE}
predict(tree, unseenCustomer, type="class")
```


# Summary

# Literature
