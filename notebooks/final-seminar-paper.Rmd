---
title: "Data analysis: ebuy's sales data"
author: "Group 3: Julius Braun (xxxxxxx), Patrick Preuß (5813166), Paul Schütz (xxxxxxx)"
date: "July 2019"
bibliography: "bibliographies/literature.bib"
output:
  bookdown::html_document2:
    toc: yes
    toc_depth: 2
    theme: spacelab
---

```{r setRngSeed, echo=FALSE}
# To always get (and report) the same cross-validation results (and any
# other random-based things) we initialize the RNG with a fixed seed:
set.seed(26769)
```

# Introduction
```{r includes01, echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(knitr)
library(kableExtra)
library(usmap)
library(ineq)
library(janitor)
library(stringr)
library(randomForest)
library(caret)
library(rpart)
library(rpart.plot)
```

## Our Approach
In this report we analysed the data of ebuy, an online sales platform. We used the systematic approach from @wickhamGrolemund2017 of importing, tidying, exploring (transform, visualise, model), and communicating (figure \@ref(fig:wickham2017Explor) for data sets provided by ebuy itself.

```{r wickham2017Explor, echo=FALSE, fig.cap="Grolemund and Wickham (2017): Data Exploration", fig.align = 'center'}
include_graphics("images/wickham2017-data-science-explore.png")
```

## The Data
Ebuy's data came separated into different files: files that contained the acutal observations and files that described the columns. We first joined the column names with their respective data sets and after examining the data we than proceeded to clean them. This included eliminating empty rows and columns, as well as renaming and forcing some of the variables to be categorical.
```{r unzipClickstream1, echo=FALSE, message=FALSE}
# The first part of the clickstream data comes in a ZIP archive. We have to unzip it first.
# This was done in 'src/data/script_01_unzip_data.sh' before. Its now part of this "big markdown file".

unzip("../data/raw/clickstream/clickstream_data.zip", exdir="../data/interim/clickstream/")
```

```{r loadingTheData, echo=FALSE, message=FALSE}
# This code chunk used to reside in 'src/data/script_02_join_data_with_column_names.r'! Its now part of
# the "big markdown file".
readdatawithcolumns <- function(headersFile, dataFile) {
    message("Reading headers...")
    tmpHeaderNames <- read.csv(file=headersFile, sep=":", header=FALSE)
    headerNames <- tmpHeaderNames$V1

    message("Replacing spaces...")
    headerNames <- gsub(" ", "_", headerNames)

    message("Adding headers to data file...")
    df <- read.csv(file=dataFile, sep=",", header=FALSE)
    colnames(df) <- headerNames
    return(df)
}

# read the ORDER data:
orderDf <- readdatawithcolumns(headersFile="../data/raw/orders/order_columns.txt",
                               dataFile="../data/raw/orders/order_data.csv")
write.csv(orderDf, file="../data/interim/orders/orders_with_headers.csv", row.names=FALSE)

# read the CLICKSTREAM:
clickstreamDf1 <- readdatawithcolumns(headersFile="../data/raw/clickstream/clickstream_columns.txt",
                                      dataFile="../data/interim/clickstream/clickstream_data.csv")
clickstreamDf2 <- readdatawithcolumns(headersFile="../data/raw/clickstream/clickstream_columns.txt",
                                      dataFile="../data/raw/clickstream/clickstream_data_part_2.csv")
clickstreamDf <- rbind(clickstreamDf1, clickstreamDf2)
write.csv(clickstreamDf, file="../data/interim/clickstream/clickstream_with_headers.csv", row.names=FALSE)
```

```{r cleaningTheData, echo=FALSE, message=FALSE}
# This code chunk used to reside in 'src/data/script_03_clean_data.r'! Its now part of the "big markdown file".
cleanFile <- function(filePath, threshold, keepColumns) {
  message(sprintf("Loading file from %s.", filePath))
  df <- read.csv(file=filePath, header=TRUE, na.strings = c("?", "NA", "NULL"))

  message("Cleaning data.")
  message("1. Removing empty rows and columns.")
  dfCleaned <- remove_empty(dat = df, which = c("rows", "cols"))
  columnsDeleted <- ncol(df) - ncol(dfCleaned)
  message(sprintf("1. Removed %d columns", columnsDeleted))

  message("2. Removing constant columns.")
  dfCleaned <- remove_constant(dat = dfCleaned, na.rm = TRUE)
  columnsDeleted <- ncol(df) - ncol(dfCleaned) - columnsDeleted
  message(sprintf("2. Removed %d columns", columnsDeleted))

  message(sprintf("3. Removing columns with unknown values (threshold = %f).", threshold))
  deletableColumns <- list()
  for(c in 1:ncol(dfCleaned)){
    currentColumn <- dfCleaned[,c]
    unknownPercentage <- sum(is.na(currentColumn))/length(currentColumn)
    if(colnames(dfCleaned)[c] %in% keepColumns){
      next
    }
    if(unknownPercentage >= threshold){
      deletableColumns <- c(deletableColumns, colnames(dfCleaned)[c])
    }
  }

  columnsDeleted <- length(deletableColumns)
  message(sprintf("3. Removed %d columns", columnsDeleted))

  dfCleaned <- dfCleaned[, !(names(dfCleaned) %in% deletableColumns)]

  message(sprintf("File cleaned. Total deleted columns: %d", ncol(df)-ncol(dfCleaned)))
  return(dfCleaned)
}

clickstream_data <- cleanFile("../data/interim/clickstream/clickstream_with_headers.csv", 0.85, c("US_State", "Customer_ID"))
orderDfCleaned <- cleanFile("../data/interim/orders/orders_with_headers.csv", 0.85, c())


```

```{r pickColumnsForceCat01, echo=FALSE}
orderDfCleaned$Product_Family_ID <- as.factor(orderDfCleaned$Product_Family_ID)
orderDfCleaned$Order_Line_Day_of_Week <- as.factor(orderDfCleaned$Order_Line_Day_of_Week)
orderDfCleaned$Order_Line_Hour_of_Day <- as.factor(orderDfCleaned$Order_Line_Hour_of_Day)
orderDfCleaned$Gender <- as.factor(orderDfCleaned$Gender)
orderDfCleaned$US_State <- as.factor(orderDfCleaned$US_State)
orderDfCleaned$Age <- as.numeric(orderDfCleaned$Age)
```

# Summary Statistics
## Clicks

The data period contains data for 17 days. To get an overview over the clickstream data, we look at different measures in the data. The table below shows how many sessions and requests Ebuy's web site accumulated per day. The last measure is the average requests per session which makes it easier to compare the average session length between days. Apparently, the data sampling started on April 14th, 2000 in the late evening because there are only 37 requests in five sessions, while every other day generated at least 6800 requests.
Also, there are no clear correlations between the day of the week and the session length, but there seems to be a correlation between the day of the week and session count. On weekends, there are less sessions and requests than during workdays. Whether this correlation is significant can be investigated by deeper analysis.

```{r requestsummary, echo=FALSE}

clickstream_data %>%
  group_by(Request_Date) %>%
  summarise(Day = first(REQUEST_DAY_OF_WEEK), Sessions = n_distinct(Session_ID), Requests = n(), "Requests per Session" = Requests/Sessions) %>%
  rename(Date = Request_Date) %>%
  kable() %>%
  kable_styling(full_width=FALSE)

```

After we find out when and how long visitors browse on Ebuy's web site, we want to know what products they look at. Because of the large set of products, we only look at the top 10 products of Ebuy's assortment. Since the clickstream data does not provide information about the product names, we have to choose the product ID. The "Views per Session" measure shows how often the product is viewed per session in which it the product site is visited. By far the most prevalent product is the product with ID 10315. It is also the product with the least views per session, which means that it is relatively also viewed in the most sessions. Thus, it is truly the most popular product in the clickstream data. Also, it accumulated more than twice as many requests as the product with ID 12483, which is the sixth-most viewed product. That shows how large the difference is even between the top products.

```{r productrequests, echo=FALSE}

clickstream_data %>%
  group_by(Product_ID) %>%
  summarise(Requests = n(), Sessions = n_distinct(Session_ID), Views_Per_Session = Requests/Sessions) %>%
  arrange(desc(Requests)) %>%
  filter(!is.na(Product_ID)) %>%
  top_n(wt = Requests, n=10) %>%
  rename("Product ID" = Product_ID, "Views per Session" = Views_Per_Session) %>%
  kable() %>%
  kable_styling(full_width=FALSE)
  

```

Another perspective on the clickstream data is the customer. The question is which customers are the most regular visitors of the Ebuy web site and how long they stay on the site. The information has to be evaluated with caution, because the data only lists customers that are logged into the web site. The customer with the most requests has the customer ID 5224 with 247 requests over the data period. Also, customer 5224 has six sessions in the 17 days of data sampling, which means the customer visits the site about every third day. Within the top 10 customers, there are three customers with only one session and a high "pages per session" value. 

```{r customerrequests, echo=FALSE}

clickstream_data %>%
  group_by(Customer_ID) %>%
  summarise(Requests = n(), Sessions = n_distinct(Session_ID), Pages = n()/n_distinct(Session_ID)) %>%
  arrange(desc(Requests)) %>%
  filter(!is.na(Customer_ID)) %>%
  top_n(wt = Requests, n=10) %>%
  rename("Customer ID" = Customer_ID, "average pages per session" = Pages) %>%
  kable() %>%
  kable_styling(full_width=FALSE)

```


## Customers

It is always important to know who your customers are when conducting a business. Therefore we looked at some of the most apparent properties of your customers. First we looked on how old ebuy's customers are. Your youngest customer is 18 years old while your oldest customer indeed is 98 years old. Although it is nice to see that there are some very old people who know how to operate a computer properly for ebuy it is more intersting to see that your average customer is in his/her mid 30's. Maybe ebuy should research what trends are emerging in this age segment so you can fit your offerings to your audience.

```{r ageDistro, echo = FALSE}
filtered = orderDfCleaned %>%
  na_if("?")  %>%
  distinct(Customer_ID, .keep_all = TRUE)

summary(as.numeric(as.character(filtered$Age)))

```

When looking at the gender distribution in your customer base it is interesting to observe that most of your customers are women. Maybe ebuy should consider putting some marketing in place to attract more men to the platform or fully commit to only sell products for women.

```{r genderDistro, echo = FALSE}
filtered = orderDfCleaned %>%  na_if("NULL")  %>% distinct(Customer_ID, .keep_all = TRUE)
summary(as.factor(as.character(filtered$Gender)))

```

Furthermore we was interested into how people get to know about ebuy and it turns out that most of ebuy's customers got aware of ebuy through they friends and family in the first place. It seems like ebuy is really dependent on mouth to mouth propaganda. E-mail advertisements are also working quite fine in generating new leads, whereas only a fraction of customers got interested into ebuy via print ads. Maybe ebuy should look closer into social media marketing and other types of modern marketing mechanisms if you want to get more independent from mouth to mouth propaganda which is hard to control although it is working fine for now.

```{r howDidYouHearAboutUs, echo = FALSE}
filtered = orderDfCleaned %>%  na_if("?")  %>% distinct(Customer_ID, .keep_all = TRUE)
summary(as.factor(as.character(filtered$HowDidYouHearAboutUs)))

```

We also tried to get insight about how many of your customers are parents. If the share is high maybe it makes sense to also offer products for babies and young children since parents buy the clothing for their kinds until a specific age of the children. Maybe parents will just put a new pyjama for their kid in the basket while shopping a new dress for themselve. It turns out that indeed approximatively 40% of your  customers are parents.

```{r presenceOfChildren, echo = FALSE}
filtered = orderDfCleaned %>% na_if("?")  %>% distinct(Customer_ID, .keep_all = TRUE)
summary(as.factor(as.logical(filtered$Presence_Of_Children)))

```

## Products {#products_summary}
For ebuy it should be interesting to find out, which of their products and product families are the ones that sell best. The best selling product is a pair of socks from brand HOSO, their best selling product family has the ID 12295. Unfortunately the data does not provide any meaningful further details, what this product family is made of.
```{r mostOrderedProductsAndProductFamilies, echo=FALSE}
findDistinctBrandname <- function(dataFrame, productId) {
  bn <- dataFrame %>%
    filter(Product_ID == productId) %>%
    select(BrandName) %>%
    distinct(BrandName) %>%
    slice(1:1)
  return(bn$BrandName)
}

topSellingProducts <- orderDfCleaned %>%
  select(Product_ID, BrandName) %>%
  drop_na(Product_ID) %>%
  count(Product_ID) %>%
  arrange(desc(n)) %>%
  rowwise() %>%
  mutate(
    brandName = findDistinctBrandname(orderDfCleaned, Product_ID)
  )

topSellingProducts %>%
  slice(1:5) %>%
  kable(.) %>%
  kable_styling(full_width=FALSE)

orderDfCleaned %>%
  drop_na(Product_Family_ID) %>%
  count(Product_Family_ID) %>%
  arrange(desc(n)) %>%
  slice(1:5) %>%
  kable(.) %>%
  kable_styling(full_width=FALSE)
```


# Plots
## Clicks
### How are the requests distributed geographically?

Since all available clickstream data show requests from the US market, it is interesting to see how visitors from different US states behave on the web site. Figure \@ref(fig:heatmap) shows how many requests per US state have been recorded in the data time span. While the information of the figure is extremely useful, it has to be noted that `r format(round(100*sum(is.na(clickstream_data$US_State))/length(clickstream_data$US_State), 2), nsmall = 2)`% of the observations in the clickstream data do not include information about the state of the visitor. Still, `r sum(!is.na(clickstream_data$US_State))` entries contain the US state, making the plot meaningful.

```{r heatmap, echo=FALSE, fig.cap="Heatmap of requests per US state", warning=FALSE}


map_data <- clickstream_data %>%
  select(US_State) %>%
  group_by(US_State) %>%
  summarise(Request_Count = n(), state = first(US_State))

plot_usmap(data = map_data, values = "Request_Count", labels = TRUE) +
  scale_fill_continuous(low = "white", high = "red", name = "Request Count") +
  labs(title = "How many requests come from each US state?") +
  theme(plot.title = element_text(face = "bold", size = 15, hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5, size = 12, face = "italic"),
        legend.position = "bottom", legend.justification = "center",
        legend.background = element_blank())

```

First of all, it stands out that there is no recorded data for Wyoming and Vermont. Although it is very likely that there were unrecorded requests from these states, we cannot make assumptions or predictions for the states on this basis.
For the other states however, the plot can be used to draw conclusions.
The states with the most requests are New York (`r map_data$Request_Count[map_data$state == "NY"]` requests), California (`r map_data$Request_Count[map_data$state == "CA"]` requests) and Texas (`r map_data$Request_Count[map_data$state == "TX"]` requests). Compared to the average of `r format(round(mean(map_data$Request_Count[!is.na(map_data$state)]), 2), nsmall = 2)` requests per state, these three states are overproportionally represented. Reasons for this high number of requests could be that this states are the states with the highest Gross Domestic Product (GDP) in the year 2000 ([U.S. Bureau of Economic Analysis](https://apps.bea.gov/iTable/iTable.cfm?reqid=70&step=30&isuri=1&major_area=0&area=01000,02000,04000,05000,06000,08000,09000,10000,11000,12000,13000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,44000,45000,46000,47000,48000,49000,50000,51000,53000,54000,55000,56000,91000&year=2000&tableid=505&category=1505&area_type=0&year_end=-1&classification=naics&state=0&statistic=1&yearbegin=-1&unit_of_measure=levels)).
The implications for Ebuy are to especially target visitors from New York, California and Texas to generate profit from the high ratio of visitors coming from these states. Targeting methods could include special offers or extensive marketing.

```{r createRequestData, echo=FALSE}
data_requests_products <- clickstream_data %>%
  select(Product_ID) %>%
  filter(Product_ID != "?") %>%
  group_by(Product_ID) %>%
  summarise(Request_Count = n())

```

### How many times was each product accessed?

Another important information is which products are viewed most often. Figure \@ref(fig:requests) shows the distribution of requests per product. Although there are too many products to show the individual number of requests per product, it becomes clear that the requests are not equally distributed. The distribution has the form of a long-tail curve, meaning that a small portion of all products get a large proportion of the requests. The implications for Ebuy are that not all products can be treated equally. Ebuy should focus their marketing activities on products that are viewed more often in order to generate more profit. The most viewed product has product ID `r data_requests_products[order(data_requests_products$Request_Count, decreasing = TRUE),][1,1]` and was visited `r data_requests_products[order(data_requests_products$Request_Count, decreasing = TRUE),][2,1]` times. Compared with the IDs of the most bought products in \@ref(products_summary), it becomes apparent that the most viewed product is not the most bought one. Since it is the most viewed product, it has to be interesting for potential customers. Ebuy should investigate what keeps customers from buying that product because they are missing out on profit. Possible reasons for that could be that the price is too high and that customers can buy an equal product cheaper somewhere else, or that the presentation of the product on the web site is not appealing enough to convince visitors to buy it.

```{r requests, echo = FALSE, fig.cap="Requests per product"}

ggplot(data_requests_products) +
  geom_line(aes(x = reorder(Product_ID, desc(Request_Count)), y = Request_Count, group = 1)) +
  labs(x = "Products", y = "Requests")  +
  theme_classic() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) +
  scale_y_continuous(expand = c(0,0))

```

### How long do customers stay on the Ebuy web site?

To analyze how the average session on the Ebuy web site looks like, we look at the length of the session in two different ways: The first variable is the number of different pages visited during a single session. The second variable is the time between the first and the last request of the session. The scatterplot in figure \@ref(fig:sessionduration) shows how the sessions are distributed regarding the previously mentioned dimensions.
The first observation is that generally speaking the average session duration in seconds increases with an increasing number of pages visited. This finding is to be expected for normal online shopping behavior. However, the average session duration becomes more scattered with higher pages visited. This can be explained with the fact that a smaller proportion of sessions shows a high amount of pages visited, making outliers more impactful to the average than in the data points with small numbers of visited pages.
The second observation is that most visitors look at less than ten pages in a session. That shows that only a few visitors use Ebuy's web site to browse through different available products. Most visitors seem to have a clear goal when entering the site and only look at the product (and maybe a few alternative products) and then decide whether they buy that product or not. Ebuy could use improved advertisement (e.g. features like "Customers who bought this product also bought that product") to convince visitors to stay on their web site for longer and thus be more likely to buy more than one product.
The previous finding could also indicate that there are many regular customers who always buy the same product and thus only stay on the web site for a short period of time since they know what product they want.

```{r sessionduration, echo=FALSE, fig.cap="Scatterplot of session characteristics"}
  duration_data <- clickstream_data %>%
  select(Session_ID, Request_Sequence, Request_Date, Request_Date_Time, Session_First_Request_Date, Session_First_Request_Date_Time) %>%
  unite(col = "Request_Timestamp",c("Request_Date", "Request_Date_Time"), sep = " ", remove = TRUE) %>%
  unite(col="Session_First_Request_Timestamp", c("Session_First_Request_Date","Session_First_Request_Date_Time"), sep=" ", remove=TRUE) %>%
  mutate(Request_Timestamp = as.POSIXct(Request_Timestamp, format="%Y-%m-%d %H\\:%M\\:%S")) %>%
  mutate(Session_First_Request_Timestamp = as.POSIXct(Session_First_Request_Timestamp, format="%Y-%m-%d %H\\:%M\\:%S")) %>%
  group_by(Session_ID) %>%
  summarise(Pages_Visited=n(),Session_First_Request=min(Session_First_Request_Timestamp),Session_Last_Request=max(Request_Timestamp)) %>%
  mutate(Session_Duration = as.numeric(difftime(Session_Last_Request, Session_First_Request,units = "secs",))) %>%
  group_by(Pages_Visited) %>%
  summarise(avg_Session_Duration = mean(Session_Duration), Session_Count = n_distinct(Session_ID)) %>%
  filter(Session_Count > 2 & Pages_Visited > 1)

  ggplot(data = duration_data) +
  geom_point(aes(x = Pages_Visited, y = avg_Session_Duration, size = Session_Count)) +
  labs(x = "Pages Visited", y = "Average Session Duration (seconds)", size = "Sessions") +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

## Customers

### How is the sales volume distributed among customers?

We wanted to know if there is maybe a little group of customers which is responsible for large parts of the overall spendings on ebuy. If this is the case it is probably important to focus on such a group as they are generating most of your revenue. Therefore we looked at how the spendings on ebuy are distributed among the customers. The lorenz curve in figure \@ref(fig:calculateMoneySpendPerCustomer) vizualizes this distribution.
As you can see the curve shows that the distribution of sales is not equally distributed among the customers. The curve's gini coefficient has a value of 0.46 indicating an very unequal distribution. Interestingly the upper ten percent of customers are responsible for approximately 42% of total sales. This tells us that indeed there is a  customer group which spends much more on ebuy than the remaining majority. We suggest that ebuy should look take attention on this group and foster their willingness to spend money on ebuy, maybe with exclusive offerings for royality customers.

```{r calculateMoneySpendPerCustomer, echo=FALSE, fig.cap="Lorenz Curve - Customer Spendings"}
money_spent_per_customer <- orderDfCleaned %>%
  group_by(Customer_ID) %>%
  summarise(Total_Spending = sum(Order_Line_Amount))

lorenz_money_spent_per_customer <- Lc(money_spent_per_customer$Total_Spending)
lorenz_money_spent_per_customer_df <- data.frame(p=lorenz_money_spent_per_customer$p, L=lorenz_money_spent_per_customer$L)

ggplot(data=lorenz_money_spent_per_customer_df) +
  geom_line(aes(x=p, y=L)) +
  scale_x_continuous(name="Cumulative share of Customers", limits=c(0,1)) +
  scale_y_continuous(name="Cumulative share of Spendings", limits=c(0,1)) +
  geom_abline(color = "grey")
```

### How are customer spendings distributed among the different brands?

We also looked into whether people are spending their money equally for all brands or if spendings are focussed on a particular group of brands. The lorenz curve in figure \@ref(fig:lorenzBrandToSales) has a gini coefficient of 0.36 indicating a quite inequal distribution of sales to the respective brands. The upper 10 percent of brands are responsible for approximately 28% of generated sales while the lower 50 percent of the brands are only responsible for 21% of sales. Maybe ebuy should consider to boost some of the very successful brands because people tend to buy more of it. Ebuy could show new customers products of one of the successful brands when they first visit the ebuy page because they will more likely buy it which will in turn increase sales. In the following we will look into which are the most successful brands for men and women.

```{r lorenzBrandToSales, echo = FALSE, fig.cap="Lorenz Curve - Brand Spendings" }
orders_with_brand <- orderDfCleaned
orders_with_brand$Brand = unlist(lapply(orderDfCleaned$Product_Level_2_Path, FUN = function(path){return(str_match(path, "\\/.*\\/.*\\/(.*)")[,2])}))

money_spent_per_brand <- orders_with_brand %>%
  group_by(Brand) %>%
  summarise(Total_Spending = mean(Order_Line_Amount))


lorenz_money_spent_per_brand <- Lc(money_spent_per_brand$Total_Spending)

lorenz_money_spent_per_brand_df <- data.frame(p=lorenz_money_spent_per_brand$p, L=lorenz_money_spent_per_brand$L)

ggplot(data=lorenz_money_spent_per_brand_df) +
  geom_line(aes(x=p, y=L)) +
  scale_x_continuous(name="Cumulative share of Brands", limits=c(0,1)) +
  scale_y_continuous(name="Cumulative share of Spendings", limits=c(0,1)) +
  geom_abline(color = "grey")
```

### What are the most popular brands for men and women?

```{r include=FALSE}
orders = orderDfCleaned %>%
  select(Customer_ID, Order_Line_Session_ID, Order_ID, Customer_ID,
         Age, Gender, Product_Level_2_Path, Order_Status, Order_Line_Quantity)
orders$Brand = unlist(lapply(orders$Product_Level_2_Path, FUN = function(path){return(str_match(path, "\\/.*\\/.*\\/(.*)")[,2])}))
```

To get an overview over what brands your customers like the most we vizualized the popularity of the brands based on how many articles was bought from the brand. Hanes and American Essentials are the most popular brands among ebuy's female customers (compare figure \@ref(fig:brandsFemale)). Interestingly those are also the two most successful brands for men as you can see in figure \@ref(fig:brandsMale)) although men bought products from American Essential twice as often as from Hanes.

```{r prepareBrandData, include= FALSE}
mostPopularFemaleBrands = orders %>%
  filter(Gender == "Female") %>%
  filter(!is.na(Brand)) %>%
  group_by(Brand) %>%
  summarise(Order_Amount = sum(Order_Line_Quantity)) %>%
  arrange(desc(Order_Amount)) %>%
  head(10)
```

```{r brandsFemale, echo=FALSE, fig.cap= "Most Popular Brands for Females (based on cumulated order amount)"}
ggplot(mostPopularFemaleBrands, mapping = aes(x = reorder(Brand, Order_Amount), y = Order_Amount, fill = Brand)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Brand", y = "Order Amount")
```

```{r include=FALSE}
mostPopularMaleBrands = orders %>%
  filter(Gender == "Male") %>%
  filter(!is.na(Brand)) %>%
  group_by(Brand) %>%
  summarise(Order_Amount = sum(Order_Line_Quantity)) %>%
  arrange(desc(Order_Amount)) %>%
  head(10)
```

```{r brandsMale, echo=FALSE, fig.cap="Most Popular Brands for Males (based on cumulated order amount)"}
ggplot(mostPopularMaleBrands, mapping = aes(x = reorder(Brand, Order_Amount), y = Order_Amount, fill = Brand)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Brand", y = "Number of Purchases")
```

## Orders
### Different order volumes on different days of the week?
Orders at ebuy's online shop might be distributed differently among the different days of the week. To examine, whether this is the case, we plotted a frequency distribution of all orders over the days of week. We first reordered the days in the canonical way (i.e. Monday, Tuesday, Wednesday, ...) and then counted and plotted the order frequencies against them (figure \@ref(fig:orderDayBarPlot1)). Clearly, the most orders are placed during the weekdays (peaking on Wednesdays), the least orders on sunday. In order to ramp up their sales on those days where sales are particulary low (Saturday, Sunday, Monday), ebuy could for instance try to hand out (digital) coupons that are only valid on the weekend, therby hopefully increasing their sales figures.
```{r orderDayBarPlot1, echo=FALSE, fig.cap="Distribution of orders among different days of the week", fig.align = 'center'}
orderDfCleaned %>%
  mutate(
    Order_Day_of_Week = factor(Order_Day_of_Week, levels=c(
      "Monday",
      "Tuesday",
      "Wednesday",
      "Thursday",
      "Friday",
      "Saturday",
      "Sunday"))
    ) %>%
  count(Order_Day_of_Week) %>%
  slice(1:7) %>%
  ggplot(data = .) +
    geom_bar(mapping = aes(x = Order_Day_of_Week, y = n), stat = "identity")
```

### What is the distribution of order amounts?
For an online shop like ebuy it might be of interest to identify those customers that place the vast majority of all orders. In order to visualize the distribution of order amounts we computed their Lorenz curve and plotted the cummulative percentages (figure \@ref(fig:orderTotalLc)). The curve shows a highly inequal distribution of order amounts: the lower 75% of all orders only account for roughly 25% of the total order amounts, while a quarter of all orders make up for three quarters of the sales total.
```{r orderTotalLc, echo=FALSE, fig.cap="Lorenz Curve: Distribution of order amounts", fig.align = 'center'}
lc <- Lc(orderDfCleaned$Order_Amount,
         n = rep(1, length(orderDfCleaned$Order_Amount)),
         plot = F)

p <- lc[1]
L <- lc[2]
lc <- data.frame(p,L)

ggplot(data = lc) +
  geom_line(aes(x = p, y = L)) +
  scale_y_continuous(name="% of order amounts", limits=c(0,1)) +
  scale_x_continuous(name="% of orders", limits=c(0,1)) +
  geom_abline()
```

# Ebuy's Experiment
## Data and Summary Statistics
Ebuy's experiment provided us with data regarding different recommender systems. After loading and examining the dataset we picked all columns of interest (e.g. deciding to keep the first ID column only) and recoded the chosen ID column as a factor. We than calculated basic summary statistics, where the mean values where of most interest.
```{r loadexpdata, echo=FALSE}
experimentDf <- read.csv(file="../experiment/experimental_results.csv", sep=",", na.strings=c("NA"))
experimentDf <- experimentDf[c("id_", "ranking_based", "random_recommendations", "profit_oriented")]
names(experimentDf)[1] <- "id"
experimentDf$id <- as.factor(experimentDf$id)
summary(experimentDf)
```

## Broad Overview of Distributions

When taking a broad look on the means and densities of the different recommender systems, you can clearly see that the profit-oriented recommender system has the highest mean sales with an average of 21.75€ per customer (see figure \@ref(fig:profitDensityAndMean)) in contrast to an average of  19€ per customer for the ranking-based recommender (\@ref(fig:rankingDensityAndMean)) and 18.50€ for the random recommender (see figure \@ref(fig:randomDensityAndMean)). It is also worth to mention that the density for the profit-based recommender system is pretty narrow (compare figure  \@ref(fig:profitDensityAndMean)), indicating a lower standard deviation and probably a tighter confidence interval than the other two wider distributions for the ranking-based (see figure \@ref(fig:rankingDensityAndMean)) and the random recommender systems (see figure \@ref(fig:randomDensityAndMean)).

```{r recommenderSetup, include=FALSE}
recommender_to_sales <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("Id", "Recommender", "Sales"))
index <- 1
for(row in 1:nrow(experimentDf)){
  if(!is.na(experimentDf[row, "ranking_based"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "ranking_based",
                                                                as.numeric(experimentDf[row, "ranking_based"]))
    index <- index+1
  }
  if(!is.na(experimentDf[row, "random_recommendations"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "random_recommendations",
                                                                as.numeric(experimentDf[row, "random_recommendations"]))
    index <- index+1
  }
  if(!is.na(experimentDf[row, "profit_oriented"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "profit_oriented",
                                                                as.numeric(experimentDf[row, "profit_oriented"]))
    index <- index+1
  }
}
recommender_to_sales$Id <- NULL
```

```{r rankingDensityAndMean, echo=FALSE, fig.cap="Ranking-Based Recommender - Sales Density and Mean"}
ggplot(recommender_to_sales %>% filter(Recommender=="ranking_based"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density")
```

```{r profitDensityAndMean, echo=FALSE, fig.cap="Profit-Oriented Recommender - Sales Density and Mean"}
ggplot(recommender_to_sales %>% filter(Recommender=="profit_oriented"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density")
```

```{r randomDensityAndMean, echo=FALSE, fig.cap="Random Recommender - Sales Density and Mean"}
ggplot(recommender_to_sales %>% filter(Recommender=="random_recommendations"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density")
```

When taking a closer look at the confidence intervals of the different recommender systems (see figure \@ref(fig:confidenceIntervalsRecommenders)) you can clearly  see that the profit-based recommender systems shows the best performance and also has a very tight 95% confidence interval indicating high certainty in the amount of profit made from the recommender per customer. On the other side the confidence intervals for the profit-based recommender and the random recommender system are more wide and also overlap each other.

```{r confidenceIntervalsRecommenders, echo=FALSE, fig.cap="Recommender Confidence Intervals"}
result <- group_by(recommender_to_sales, Recommender) %>%
  summarise(
    Count = n(),
    Mean = mean(Sales, na.rm = TRUE),
    Sd = sd(Sales, na.rm = TRUE),
    Se = Sd/sqrt(Count),
    CiMult = qt(0.975, Count-1),
    Ci = Se * CiMult
  )

# visualize CI
ggplot(result, aes(x=Recommender, y=Mean, group=1)) +
  geom_point(alpha=0.52) +
  geom_errorbar(width=.1, aes(ymin=Mean-Ci, ymax=Mean+Ci), colour="darkred") + labs(x="Recommender System",y= "Sales")
```

Therefore we should definitely do a statisitcal test to prove if the means of the different recommender systems are significantly different from each other or not.

## Shapiro-Wilk Normality Test

Before conducting a t-test to test if the profit means of the different recommender systems are significantly different from each other or not we will first check if their populations are normally distributed, which is a necessary precondition for conducting a t-test. In order to do so we used the Shapiro-Wilk normality test which tests the null-hypothesis that the population is normally distributed.

```{r echo = FALSE}
with(recommender_to_sales, shapiro.test(Sales[Recommender == "profit_oriented"]))
with(recommender_to_sales, shapiro.test(Sales[Recommender == "ranking_based"]))
with(recommender_to_sales, shapiro.test(Sales[Recommender == "random_recommendations"]))
```
It turns out that all the p-values are smaller than the significance level of 0.05 which means that we have to refuse the null-hypothesis which in turn implies that the distributions are not normally distributed. Therefore we used the Unpaired Two-Samples Wilcoxon Test (aka Mann-Whitney U Test) to test if the means of the different recommenders are equal or not. The Mann-Whitney U Test is an valid alternative to the t-test in szenarios in which the requirements for the t-test are not met and is widely used in practice.

## Mann-Whitney U Test

```{r echo = FALSE}
# perform wulxoc test
ranking_based_sales <- recommender_to_sales %>% filter(Recommender == "ranking_based")
profit_oriented_sales <- recommender_to_sales %>% filter(Recommender == "profit_oriented")
random_recommendations_sales <- recommender_to_sales %>% filter(Recommender == "random_recommendations")

wilcox.test(ranking_based_sales$Sales, random_recommendations_sales$Sales, alternative = "two.sided")
wilcox.test(profit_oriented_sales$Sales, random_recommendations_sales$Sales, alternative = "two.sided")
```

We first tested if there is a significant difference in the means of the random recommender system and the ranking-based recommender system. Our earlier observations of the confidence intervals of the two recommenders revealed that their confidence intervals are both quite wide and also overlap at large parts (see figure \@ref(fig:confidenceIntervalsRecommenders)). The test now proves that in fact the both means of the two recommenders do not differ significantly (p = 0.2008 > 0.05).
Although the confidence intervals in figure \@ref(fig:confidenceIntervalsRecommenders) already indicate that the means of the profit-based recommender is significantly different from the random recommender system we conducted a test for that. The result of the test proves our suggestions with an p-value less than 0.001.
As the mean of the ranking-based recommender system has been shown to not differ significantly from the random recommendersystem we have to choose between the profit-oriented recommender system and the random recommender system. As the profit-oriented recommender systems mean value (21.75) is greater than the baseline mean (17.91), ebuy should preferably use the profit-oriented one.

# Models and Prediction
## Training the Model{#subsection-ttm}
As ebuy's products fall into certain product families, we trained a model that allows to predict for a new customer from which product family he will purchase a product. We first grew a large tree with a complexity parameter of cp=0.00001 (i.e. splits had to decrease the lack of fit by a factor of 0.00001). We then examined the results of a 10-fold cross-validation by plotting the complexity parameter vs. its cross-validated error (as shown in figure \@ref(fig:cpplottree1)).
```{r cpplottree1, echo=FALSE, fig.cap="Product Family ID: Complexity parameter vs. cross-validated error", fig.align = 'center'}
tree <- rpart(Product_Family_ID ~ Order_Line_Day_of_Week + Gender + Age,
              method="class",
              data=orderDfCleaned,
              cp=0.00001)

plotcp(tree)
```

To prevent over-fitting, @kabacoff2015 suggests to choose the leftmost cp value below the dotted line (here: cp=0.0007): a tree of that size (here: 45 splits) is the smallest tree whose cross-validated error is whithin one standard error of the minimum cross-validated error value. We therfore pruned the tree to its new size and pretty-printed the resulting tree (figure \@ref(fig:prunedtree1); zoomed-in partial figure \@ref(fig:tree1Zoom1)).
```{r prunedtree1, echo=FALSE, fig.cap="Product Family ID: Pruned Classification Tree", fig.align = 'center'}
prune(tree, cp=0.0007) %>%
  prp(., tweak = 1.5,)
```

```{r tree1Zoom1, echo=FALSE, fig.cap="Partially Enlarged Classification Tree", fig.align = 'center'}
# The rpart package guesses (!) the font and line/node width and height based on your operation system's
# resolution. This might cause prp() to render an ugly plot. We've chosen a "tweak" factor of 1.5 which produces
# a reasonably good looking graphical representaion of our tree. We also added this "zoomed-in" screenshot that
# (as it is an image) does not change - no matter what your resolution/DPI is.
include_graphics("images/decision-tree-zoom-01.png")
```

Next we computed a 10-fold cross-validated linear regression model that helped us understand, how the different properties of customers affected the amount of money they would spend in total.
```{r linReg1, echo=FALSE}
ctrl <- trainControl(method = "cv",
                     number = 10)

linreg1 <- train(Order_Line_Amount ~ Age + Gender,
                 data = orderDfCleaned,
                 trControl = ctrl,
                 na.action  = na.pass,
                 method = "lm")
linreg1
linreg1$finalModel
```

## Predictions
Having trained the models in \@ref(subsection-ttm) ebuy could predict the purchasing behaviour of future customers. Given such a previously unseen customer, ebuy could for instance now predict the product family which she is going to purchase from.
```{r prediction1, eval=FALSE}
predict(tree, unseenCustomer, type="class")
```


# Summary
Ebuy's data provided a welcome opportunity to take a data scientific look at real world data. Using the techniques learnt we could examine interesting phenomena and problems from such data.


# Literature
